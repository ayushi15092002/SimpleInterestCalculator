{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPFSr7yDJ7hclR/JkvkOTGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushi15092002/SimpleInterestCalculator/blob/master/feature_ext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sxFSHqX3fBvv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77b1dc34-ee70-44a7-b2de-b24c8a9d2a2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyEDFlib"
      ],
      "metadata": {
        "id": "MzUMqB5KmxpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install spectrum"
      ],
      "metadata": {
        "id": "Qh8GTtSRdy1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nitime"
      ],
      "metadata": {
        "id": "owuHCT8ceDgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mne"
      ],
      "metadata": {
        "id": "hZoEyrdyesCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xlsxwriter"
      ],
      "metadata": {
        "id": "fe9mNp_nqpad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyentrp"
      ],
      "metadata": {
        "id": "PML4k4e-MyOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nolds"
      ],
      "metadata": {
        "id": "tHgdv4-SqIY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install itertools"
      ],
      "metadata": {
        "id": "_1Ptmy_olfS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install EEGExtract"
      ],
      "metadata": {
        "id": "DUw9GOxZ_W-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6_0p5fIr-EhH"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import pywt\n",
        "import pyedflib\n",
        "import numpy as np\n",
        "import nolds\n",
        "from numpy.random import randn\n",
        "import mne\n",
        "from spectrum import *\n",
        "from os import listdir\n",
        "from nitime import utils\n",
        "import scipy.stats as sp\n",
        "from os.path import isfile, join\n",
        "from nitime.viz import plot_tseries\n",
        "from matplotlib import pyplot as plt\n",
        "from nitime import algorithms as alg\n",
        "from nitime.timeseries import TimeSeries\n",
        "import xlsxwriter\n",
        "from pyentrp import entropy as ent\n",
        "import itertools\n",
        "# import EEGExtract as eeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a workbook and add a worksheet.\n",
        "workbook = xlsxwriter.Workbook('Expenses01.xlsx')"
      ],
      "metadata": {
        "id": "ZrMKJK3kq1yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean_data_dir = \"/content/drive/My Drive/drdo/cleanData\"\n",
        "# read_datasetA(clean_data_dir)\n",
        "clean_data_dir = \"/content/drive/My Drive/drdo/cleanData_old/VP001/nback3.fif\"\n",
        "add_features(clean_data_dir)"
      ],
      "metadata": {
        "id": "QxOkwHSle8VC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72b4d68a-1332-4f6d-b7ce-63a2a1b7c805"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-8309728be9c1>:2: RuntimeWarning: This filename (/content/drive/My Drive/drdo/cleanData_old/VP001/nback3.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
            "  epochs = mne.read_epochs(fname=path,verbose=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs <Info | 11 non-empty values\n",
            " bads: []\n",
            " ch_names: FP1, AFF5, AFz, F1, FC5, FC1, T7, C3, Cz, CP5, CP1, P7, P3, Pz, ...\n",
            " chs: 28 EEG, 2 EOG\n",
            " custom_ref_applied: True\n",
            " dig: 31 items (3 Cardinal, 28 EEG)\n",
            " file_id: 4 items (dict)\n",
            " highpass: 1.0 Hz\n",
            " lowpass: 40.0 Hz\n",
            " meas_date: 2016-05-26 13:03:24 UTC\n",
            " meas_id: 4 items (dict)\n",
            " nchan: 30\n",
            " projs: []\n",
            " sfreq: 200.0 Hz\n",
            ">\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/metrics/_regression.py:796: UndefinedMetricWarning: R^2 score is not well-defined with less than two samples.\n",
            "  warnings.warn(msg, UndefinedMetricWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n",
            "Shape SE =  ()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_datasetA(subjectPath):\n",
        "    \n",
        "    try:\n",
        "        root, dirs, files = next(os.walk(subjectPath))\n",
        "\n",
        "        i=0\n",
        "        \n",
        "        for folder_name in dirs:\n",
        "          path_file_fif1 = subjectPath + '/' + folder_name + '/nback1.fif'\n",
        "          add_features(path_file_fif1)\n",
        "\n",
        "          path_file_fif2 = subjectPath + '/' + folder_name + '/nback2.fif'\n",
        "          add_features(path_file_fif2)\n",
        "\n",
        "          path_file_fif3 = subjectPath + '/' + folder_name + '/nback3.fif'\n",
        "          add_features(path_file_fif3)\n",
        "          i = i+1\n",
        "            \n",
        "    except StopIteration:\n",
        "        pass\n",
        "        print(\"Error ocurred:\")\n",
        "        print(\"Directory with dataset does not found!\")\n",
        "        print(\"Program will be terminated\")\n",
        "        exit(1)"
      ],
      "metadata": {
        "id": "K1YfI6TIkcl6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names = ['Fractal_Dimension','Coeffiecient_of_Variation','Mean_of_Vertex_to_Vertex_Slope','Variance_of_Vertex_to_Vertex_Slope',\n",
        "         'Hjorth_Activity','Hjorth_Mobility','Hjorth_Complexity',\n",
        "         'Kurtosis','2nd_Difference_Mean','2nd_Difference_Max',\n",
        "         'Skewness','1st_Difference_Mean','1st_Difference_Max',\n",
        "         'FFT_Delta_MaxPower','FFT_Theta_MaxPower','FFT_Alpha_MaxPower','FFT_Beta_MaxPower','Delta/Theta','Delta/Alpha','Theta/Alpha','(Delta+Theta)/Alpha',\n",
        "         '1_Wavelet_Approximate_Mean','1_Wavelet_Approximate_Std_Deviation','1_Wavelet_Approximate_Energy','1_Wavelet_Detailed_Mean','1_Wavelet_Detailed_Std_Deviation','1_Wavelet_Detailed_Energy','1_Wavelet_Approximate_Entropy','1_Wavelet_Detailed_Entropy',\n",
        "         '2_Wavelet_Approximate_Mean','2_Wavelet_Approximate_Std_Deviation','2_Wavelet_Approximate_Energy','2_Wavelet_Detailed_Mean','2_Wavelet_Detailed_Std _Deviation','2_Wavelet_Detailed_Energy','2_Wavelet_Approximate_Entropy','2_Wavelet_Detailed_Entropy',\n",
        "         '3_Wavelet_Approximate_Mean','3_Wavelet_Approximate_Std_Deviation','3_Wavelet_Approximate_Energy','3_Wavelet_Detailed_Mean','3_Wavelet_Detailed_Std _Deviation','3_Wavelet_Detailed_Energy','3_Wavelet_Approximate_Entropy','3_Wavelet_Detailed_Entropy',\n",
        "         '4_Wavelet_Approximate_Mean','4_Wavelet_Approximate_Std_Deviation','4_Wavelet_Approximate_Energy','4_Wavelet_Detailed_Mean','4_Wavelet_Detailed_Std _Deviation','4_Wavelet_Detailed_Energy','4_Wavelet_Approximate_Entropy','4_Wavelet_Detailed_Entropy',\n",
        "         '5_Wavelet_Approximate_Mean','5_Wavelet_Approximate_Std_Deviation','5_Wavelet_Approximate_Energy','5_Wavelet_Detailed_Mean','5_Wavelet_Detailed_Std _Deviation','5_Wavelet_Detailed_Energy','5_Wavelet_Approximate_Entropy','5_Wavelet_Detailed_Entropy',\n",
        "         '6_Wavelet_Approximate_Mean','6_Wavelet_Approximate_Std_Deviation','6_Wavelet_Approximate_Energy','6_Wavelet_Detailed_Mean','6_Wavelet_Detailed_Std _Deviation','6_Wavelet_Detailed_Energy','6_Wavelet_Approximate_Entropy','6_Wavelet_Detailed_Entropy',\n",
        "         '7_Wavelet_Approximate_Mean','7_Wavelet_Approximate_Std_Deviation','7_Wavelet_Approximate_Energy','7_Wavelet_Detailed_Mean','7_Wavelet_Detailed_Std _Deviation','7_Wavelet_Detailed_Energy','7_Wavelet_Approximate_Entropy','7_Wavelet_Detailed_Entropy',\n",
        "         '8_Wavelet_Approximate_Mean','8_Wavelet_Approximate_Std_Deviation','8_Wavelet_Approximate_Energy','8_Wavelet_Detailed_Mean','8_Wavelet_Detailed_Std _Deviation','8_Wavelet_Detailed_Energy','8_Wavelet_Approximate_Entropy','8_Wavelet_Detailed_Entropy',\n",
        "         '9_Wavelet_Approximate_Mean','9_Wavelet_Approximate_Std_Deviation','9_Wavelet_Approximate_Energy','9_Wavelet_Detailed_Mean','9_Wavelet_Detailed_Std _Deviation','9_Wavelet_Detailed_Energy','9_Wavelet_Approximate_Entropy','9_Wavelet_Detailed_Entropy',\n",
        "         '10_Wavelet_Approximate_Mean','10_Wavelet_Approximate_Std_Deviation','10_Wavelet_Approximate_Energy','10_Wavelet_Detailed_Mean','10_Wavelet_Detailed_Std _Deviation','10_Wavelet_Detailed_Energy','10_Wavelet_Approximate_Entropy','10_Wavelet_Detailed_Entropy',\n",
        "         '11_Wavelet_Approximate_Mean','11_Wavelet_Approximate_Std_Deviation','11_Wavelet_Approximate_Energy','11_Wavelet_Detailed_Mean','11_Wavelet_Detailed_Std _Deviation','11_Wavelet_Detailed_Energy','11_Wavelet_Approximate_Entropy','11_Wavelet_Detailed_Entropy',\n",
        "         '12_Wavelet_Approximate_Mean','12_Wavelet_Approximate_Std_Deviation','12_Wavelet_Approximate_Energy','12_Wavelet_Detailed_Mean','12_Wavelet_Detailed_Std _Deviation','12_Wavelet_Detailed_Energy','12_Wavelet_Approximate_Entropy','12_Wavelet_Detailed_Entropy',\n",
        "         '13_Wavelet_Approximate_Mean','13_Wavelet_Approximate_Std_Deviation','13_Wavelet_Approximate_Energy','13_Wavelet_Detailed_Mean','13_Wavelet_Detailed_Std _Deviation','13_Wavelet_Detailed_Energy','13_Wavelet_Approximate_Entropy','13_Wavelet_Detailed_Entropy',\n",
        "         '14_Wavelet_Approximate_Mean','14_Wavelet_Approximate_Std_Deviation','14_Wavelet_Approximate_Energy','14_Wavelet_Detailed_Mean','14_Wavelet_Detailed_Std _Deviation','14_Wavelet_Detailed_Energy','14_Wavelet_Approximate_Entropy','14_Wavelet_Detailed_Entropy',\n",
        "         '15_Wavelet_Approximate_Mean','15_Wavelet_Approximate_Std_Deviation','15_Wavelet_Approximate_Energy','15_Wavelet_Detailed_Mean','15_Wavelet_Detailed_Std _Deviation','15_Wavelet_Detailed_Energy','15_Wavelet_Approximate_Entropy','15_Wavelet_Detailed_Entropy',\n",
        "         '16_Wavelet_Approximate_Mean','16_Wavelet_Approximate_Std_Deviation','16_Wavelet_Approximate_Energy','16_Wavelet_Detailed_Mean','16_Wavelet_Detailed_Std _Deviation','16_Wavelet_Detailed_Energy','16_Wavelet_Approximate_Entropy','16_Wavelet_Detailed_Entropy',\n",
        "         '17_Wavelet_Approximate_Mean','17_Wavelet_Approximate_Std_Deviation','17_Wavelet_Approximate_Energy','17_Wavelet_Detailed_Mean','17_Wavelet_Detailed_Std _Deviation','17_Wavelet_Detailed_Energy','17_Wavelet_Approximate_Entropy','17_Wavelet_Detailed_Entropy',\n",
        "         '18_Wavelet_Approximate_Mean','18_Wavelet_Approximate_Std_Deviation','18_Wavelet_Approximate_Energy','18_Wavelet_Detailed_Mean','18_Wavelet_Detailed_Std _Deviation','18_Wavelet_Detailed_Energy','18_Wavelet_Approximate_Entropy','18_Wavelet_Detailed_Entropy',\n",
        "         '19_Wavelet_Approximate_Mean','19_Wavelet_Approximate_Std_Deviation','19_Wavelet_Approximate_Energy','19_Wavelet_Detailed_Mean','19_Wavelet_Detailed_Std _Deviation','19_Wavelet_Detailed_Energy','19_Wavelet_Approximate_Entropy','19_Wavelet_Detailed_Entropy',\n",
        "         '20_Wavelet_Approximate_Mean','20_Wavelet_Approximate_Std_Deviation','20_Wavelet_Approximate_Energy','20_Wavelet_Detailed_Mean','20_Wavelet_Detailed_Std _Deviation','20_Wavelet_Detailed_Energy','20_Wavelet_Approximate_Entropy','20_Wavelet_Detailed_Entropy',\n",
        "         '21_Wavelet_Approximate_Mean','21_Wavelet_Approximate_Std_Deviation','21_Wavelet_Approximate_Energy','21_Wavelet_Detailed_Mean','21_Wavelet_Detailed_Std _Deviation','21_Wavelet_Detailed_Energy','21_Wavelet_Approximate_Entropy','21_Wavelet_Detailed_Entropy',\n",
        "         '22_Wavelet_Approximate_Mean','22_Wavelet_Approximate_Std_Deviation','22_Wavelet_Approximate_Energy','22_Wavelet_Detailed_Mean','22_Wavelet_Detailed_Std _Deviation','22_Wavelet_Detailed_Energy','22_Wavelet_Approximate_Entropy','22_Wavelet_Detailed_Entropy',\n",
        "         '23_Wavelet_Approximate_Mean','23_Wavelet_Approximate_Std_Deviation','23_Wavelet_Approximate_Energy','23_Wavelet_Detailed_Mean','23_Wavelet_Detailed_Std _Deviation','23_Wavelet_Detailed_Energy','23_Wavelet_Approximate_Entropy','23_Wavelet_Detailed_Entropy',\n",
        "         '24_Wavelet_Approximate_Mean','24_Wavelet_Approximate_Std_Deviation','24_Wavelet_Approximate_Energy','24_Wavelet_Detailed_Mean','24_Wavelet_Detailed_Std _Deviation','24_Wavelet_Detailed_Energy','24_Wavelet_Approximate_Entropy','24_Wavelet_Detailed_Entropy',\n",
        "         '25_Wavelet_Approximate_Mean','25_Wavelet_Approximate_Std_Deviation','25_Wavelet_Approximate_Energy','25_Wavelet_Detailed_Mean','25_Wavelet_Detailed_Std _Deviation','25_Wavelet_Detailed_Energy','25_Wavelet_Approximate_Entropy','25_Wavelet_Detailed_Entropy',\n",
        "         '26_Wavelet_Approximate_Mean','26_Wavelet_Approximate_Std_Deviation','26_Wavelet_Approximate_Energy','26_Wavelet_Detailed_Mean','26_Wavelet_Detailed_Std _Deviation','26_Wavelet_Detailed_Energy','26_Wavelet_Approximate_Entropy','26_Wavelet_Detailed_Entropy',\n",
        "         '27_Wavelet_Approximate_Mean','27_Wavelet_Approximate_Std_Deviation','27_Wavelet_Approximate_Energy','27_Wavelet_Detailed_Mean','27_Wavelet_Detailed_Std _Deviation','27_Wavelet_Detailed_Energy','27_Wavelet_Approximate_Entropy','27_Wavelet_Detailed_Entropy',\n",
        "         '28_Wavelet_Approximate_Mean','28_Wavelet_Approximate_Std_Deviation','28_Wavelet_Approximate_Energy','28_Wavelet_Detailed_Mean','28_Wavelet_Detailed_Std _Deviation','28_Wavelet_Detailed_Energy','28_Wavelet_Approximate_Entropy','28_Wavelet_Detailed_Entropy',\n",
        "         '29_Wavelet_Approximate_Mean','29_Wavelet_Approximate_Std_Deviation','29_Wavelet_Approximate_Energy','29_Wavelet_Detailed_Mean','29_Wavelet_Detailed_Std _Deviation','29_Wavelet_Detailed_Energy','29_Wavelet_Approximate_Entropy','29_Wavelet_Detailed_Entropy',\n",
        "         '30_Wavelet_Approximate_Mean','30_Wavelet_Approximate_Std_Deviation','30_Wavelet_Approximate_Energy','30_Wavelet_Detailed_Mean','30_Wavelet_Detailed_Std _Deviation','30_Wavelet_Detailed_Energy','30_Wavelet_Approximate_Entropy','30_Wavelet_Detailed_Entropy',\n",
        "         'Shannon_Entropy', 'Hurst_Exponent', 'Permutation_Entropy']\n",
        "        #  ,'detrended fluctuation analysis (DFA)']"
      ],
      "metadata": {
        "id": "TeplYMB2-wRm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_features(path):\n",
        "  epochs = mne.read_epochs(fname=path,verbose=False)\n",
        "  print(\"epochs\", epochs.info)\n",
        "  nchans = epochs.info['nchan']\n",
        "  dat = []\n",
        "  for i in epochs:\n",
        "    dat.append(np.array(i))\n",
        "  \n",
        "  dat = np.array(dat)\n",
        "  # print(\"dat.shape\", dat.shape)\n",
        "  # print(\"dat.shape\",dat.shape[2])\n",
        "  tr = dat.shape[1]\n",
        "  x=[]\n",
        "  x.append(names)\n",
        "  for i in dat:\n",
        "\n",
        "    features=[]\n",
        "    # Fractal Dimension\n",
        "    features.append(fractal_dimension(i,tr))\n",
        "\n",
        "    #Coeffeicient of Variation\n",
        "    features.append(coeff_var(i,tr))\n",
        "\n",
        "    #Mean of Vertex to Vertex Slope\n",
        "    features.append(slope_mean(i,tr))\n",
        "\n",
        "    #Variance of Vertex to Vertex Slope\n",
        "    features.append(slope_var(i,tr))\n",
        "\n",
        "    #Hjorth Parameters\n",
        "    feature_list = hjorth(i,tr)\n",
        "    for feat in feature_list:\n",
        "        features.append(feat)\n",
        "\n",
        "    #Kurtosis\n",
        "    features.append(kurtosis(i,tr))\n",
        "\n",
        "    #Second Difference Mean\n",
        "    features.append(secDiffMean(i,tr))\n",
        "\n",
        "    #Second Difference Max\n",
        "    features.append(secDiffMax(i,tr))\n",
        "\n",
        "    #Skewness\n",
        "    features.append(skewness(i,tr))\n",
        "\n",
        "    #First Difference Mean\n",
        "    features.append(first_diff_mean(i,tr))\n",
        "\n",
        "    #First Difference Max\n",
        "    features.append(first_diff_max(i,tr))\n",
        "\n",
        "    #FFT Max Power - Delta, Theta, Alpha & Beta Band!\n",
        "    feature_list  =  maxPwelch(i,128,tr)            \n",
        "    for feat in feature_list:\n",
        "      features.append(feat)\n",
        "\n",
        "    #FFT Frequency Ratios\n",
        "    features.append(feature_list[0]/feature_list[1])\n",
        "    features.append(feature_list[0]/feature_list[2])\n",
        "    features.append(feature_list[1]/feature_list[3])\n",
        "    features.append((feature_list[0] + feature_list[1])/feature_list[2])\n",
        "\n",
        "    # wavlent features\n",
        "    feature_list = wavelet_features(i,nchans)\n",
        "    for feat in feature_list:\n",
        "        features.append(feat)\n",
        "\n",
        "    # Shanon Entropy\n",
        "    abc = s_entropy(i,tr)\n",
        "    print('Shape SE = ',np.shape(abc))\n",
        "    features.append(abc)\n",
        "\n",
        "    # Hurst Exponent\n",
        "    features.append(hurst(i,tr))\n",
        "\n",
        "    # Sample entropy\n",
        "    # features.append(sample_entropy(i))\n",
        "\n",
        "    # Permuation Entropy\n",
        "    features.append(permutation_entropy(i,tr))\n",
        "\n",
        "    # df analysis\n",
        "    # features.append(df_analysis(i,tr))\n",
        "\n",
        "  #   #coherence\n",
        "  #   # features.append(coherence(i))\n",
        "\n",
        "    #Spectral Entropy\n",
        "    # a = spectral_entropy(i, 128)\n",
        "    # print('Shape SE = ',np.shape(a))\n",
        "    # features.append(a)\n",
        "    \n",
        "    x.append(features)\n",
        "                    \n",
        "  # x=np.array(x)\n",
        "  # print(\"x>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
        "  # print(x)\n",
        "  import pandas as pd\n",
        "  df=pd.DataFrame(x)\n",
        "  df.to_csv(r'/content/drive/My Drive/drdo/spectral_analysis/features.xlsx',index=False)\n"
      ],
      "metadata": {
        "id": "0yjOOygWkrlJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mean(data):\n",
        "    # print(\"data\")\n",
        "    # print(data)\n",
        "    # print(\"mean\")\n",
        "    # print(np.mean(data,axis=0))\n",
        "    return np.mean(data,axis=0)"
      ],
      "metadata": {
        "id": "9PbbQnN6ghKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fractal Dimension"
      ],
      "metadata": {
        "id": "zM7-65dL_PLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fractal_dimension(Z, tr, threshold=0.9):\n",
        "\n",
        "    # Only for 2d image\n",
        "    assert(len(Z.shape) == 2)\n",
        "\n",
        "    def boxcount(Z, k):\n",
        "        S = np.add.reduceat(\n",
        "            np.add.reduceat(Z, np.arange(0, Z.shape[0], k), axis=0),\n",
        "                               np.arange(0, Z.shape[1], k), axis=1)\n",
        "\n",
        "        # We count non-empty (0) and non-full boxes (k*k)\n",
        "        return len(np.where((S > 0) & (S < k*k))[0])\n",
        "\n",
        "\n",
        "    # Transform Z into a binary array\n",
        "    Z = (Z < threshold)\n",
        "\n",
        "    # Minimal dimension of image\n",
        "    p = min(Z.shape)\n",
        "\n",
        "    # Greatest power of 2 less than or equal to p\n",
        "    n = 2**np.floor(np.log(p)/np.log(2))\n",
        "\n",
        "    # Extract the exponent\n",
        "    n = int(np.log(n)/np.log(2))\n",
        "\n",
        "    # Build successive box sizes (from 2**n down to 2**1)\n",
        "    sizes = 2**np.arange(n, 1, -1)\n",
        "\n",
        "    # Actual box counting with decreasing size\n",
        "    counts = []\n",
        "    for size in sizes:\n",
        "        counts.append(boxcount(Z, size))\n",
        "\n",
        "    # Fit the successive log(sizes) with log (counts)\n",
        "    coeffs = np.polyfit(np.log(sizes), np.log(counts), 1)\n",
        "    return -coeffs[0]\n"
      ],
      "metadata": {
        "id": "d4hgoYvS_R-D"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "coeff of var"
      ],
      "metadata": {
        "id": "CnIly8FO_Sxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def coeff_var(a,tr):\n",
        "    b = a #Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
        "    k = 0; #For counting the current row no.\n",
        "    for i in b:\n",
        "        mean_i = np.mean(i) #Saving the mean of array i\n",
        "        std_i = np.std(i) #Saving the standard deviation of array i\n",
        "        output[k] = std_i/mean_i #computing coefficient of variation\n",
        "        k=k+1\n",
        "    return np.sum(output)/tr"
      ],
      "metadata": {
        "id": "aHyXcTyu_V4q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "mean of vrtex to vertex slope"
      ],
      "metadata": {
        "id": "N4UVogkM_WjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "\n",
        "def first_diff(i):\n",
        "    b=i    \n",
        "    \n",
        "    out = np.zeros(len(b))\n",
        "    \n",
        "    for j in range(len(i)):\n",
        "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
        "        \n",
        "        j=j+1\n",
        "        c=out[1:len(out)]\n",
        "    return c #returns first diff\n",
        "\n",
        "\n",
        "def slope_mean(p,tr):\n",
        "    b = p #Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
        "    res = np.zeros(len(b)-1)\n",
        "    \n",
        "    k = 0; #For counting the current row no.\n",
        "    for i in b:\n",
        "        x=i\n",
        "        amp_max = i[argrelextrema(x, np.greater)[0]]\n",
        "        t_max = argrelextrema(x, np.greater)[0]\n",
        "        amp_min = i[argrelextrema(x, np.less)[0]]\n",
        "        t_min = argrelextrema(x, np.less)[0]\n",
        "        t = np.concatenate((t_max,t_min),axis=0)\n",
        "        t.sort()#sort on the basis of time\n",
        "\n",
        "        h=0\n",
        "        amp = np.zeros(len(t))\n",
        "        res = np.zeros(len(t)-1)\n",
        "        for l in range(len(t)):\n",
        "            amp[l]=i[t[l]]\n",
        "           \n",
        "        \n",
        "        amp_diff = first_diff(amp)\n",
        "        \n",
        "        t_diff = first_diff(t)\n",
        "        \n",
        "        for q in range(len(amp_diff)):\n",
        "            res[q] = amp_diff[q]/t_diff[q]         \n",
        "        output[k] = np.mean(res) \n",
        "        k=k+1\n",
        "    return np.sum(output)/tr\n"
      ],
      "metadata": {
        "id": "wIA9DJgx_dC7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "variance of vertex to vertex slope"
      ],
      "metadata": {
        "id": "SfYWDMoz_gwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import heapq\n",
        "from scipy.signal import argrelextrema\n",
        "\n",
        "\n",
        "def first_diff(i):\n",
        "    b=i    \n",
        "    \n",
        "    out = np.zeros(len(b))\n",
        "    \n",
        "    for j in range(len(i)):\n",
        "        out[j] = b[j-1]-b[j]# Obtaining the 1st Diffs\n",
        "        \n",
        "        j=j+1\n",
        "        c=out[1:len(out)]\n",
        "    return c #returns first diff\n",
        "\n",
        "\n",
        "def slope_var(p,tr):\n",
        "    b = p #Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) #Initializing the output array with zeros\n",
        "    res = np.zeros(len(b)-1)\n",
        "    \n",
        "    k = 0; #For counting the current row no.\n",
        "    for i in b:\n",
        "        x=i\n",
        "        amp_max = i[argrelextrema(x, np.greater)[0]]#storing maxima value\n",
        "        t_max = argrelextrema(x, np.greater)[0]#storing time for maxima\n",
        "        amp_min = i[argrelextrema(x, np.less)[0]]#storing minima value\n",
        "        t_min = argrelextrema(x, np.less)[0]#storing time for minima value\n",
        "        t = np.concatenate((t_max,t_min),axis=0) #making a single matrix of all matrix\n",
        "        t.sort() #sorting according to time\n",
        "\n",
        "        h=0\n",
        "        amp = np.zeros(len(t))\n",
        "        res = np.zeros(len(t)-1)\n",
        "        for l in range(len(t)):\n",
        "            amp[l]=i[t[l]]\n",
        "           \n",
        "        \n",
        "        amp_diff = first_diff(amp)\n",
        "        \n",
        "        t_diff = first_diff(t)\n",
        "        \n",
        "        for q in range(len(amp_diff)):\n",
        "            res[q] = amp_diff[q]/t_diff[q] #calculating slope        \n",
        "    \n",
        "        output[k] = np.var(res) \n",
        "        k=k+1#counting k\n",
        "    return np.sum(output)/tr"
      ],
      "metadata": {
        "id": "_djuIrV__nyL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hjorth"
      ],
      "metadata": {
        "id": "vO-q9zoM_oo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hjorth(input,tr):                                             # function for hjorth \n",
        "    realinput = input\n",
        "    hjorth_activity = np.zeros(len(realinput))\n",
        "    hjorth_mobility = np.zeros(len(realinput))\n",
        "    hjorth_diffmobility = np.zeros(len(realinput))\n",
        "    hjorth_complexity = np.zeros(len(realinput))\n",
        "    diff_input = np.diff(realinput)\n",
        "    diff_diffinput = np.diff(diff_input)\n",
        "    k = 0\n",
        "    for j in realinput:\n",
        "        hjorth_activity[k] = np.var(j)\n",
        "        hjorth_mobility[k] = np.sqrt(np.var(diff_input[k])/hjorth_activity[k])\n",
        "        hjorth_diffmobility[k] = np.sqrt(np.var(diff_diffinput[k])/np.var(diff_input[k]))\n",
        "        hjorth_complexity[k] = hjorth_diffmobility[k]/hjorth_mobility[k]\n",
        "        k = k+1\n",
        "    return np.sum(hjorth_activity)/tr, np.sum(hjorth_mobility)/tr, np.sum(hjorth_complexity)/tr"
      ],
      "metadata": {
        "id": "E78w0lh6_phz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "kurtosis"
      ],
      "metadata": {
        "id": "HokjM8do_s6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kurtosis(a,tr):\n",
        "    b = a # Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
        "    k = 0; # For counting the current row no.\n",
        "    for i in b:\n",
        "        mean_i = np.mean(i) # Saving the mean of array i\n",
        "        std_i = np.std(i) # Saving the standard deviation of array i\n",
        "        t = 0.0\n",
        "        for j in i:\n",
        "            t += (pow((j-mean_i)/std_i,4)-3)\n",
        "        kurtosis_i = t/len(i) # Formula: (1/N)*(summation(x_i-mean)/standard_deviation)^4-3\n",
        "        output[k] = kurtosis_i # Saving the kurtosis in the array created\n",
        "        k +=1 # Updating the current row no.\n",
        "    return np.sum(output)/tr"
      ],
      "metadata": {
        "id": "arJoYtNe_vg7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "second difference mean"
      ],
      "metadata": {
        "id": "EGNs19gY_war"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def secDiffMean(a,tr):\n",
        "    b = a # Extracting the data of the 14 channels\n",
        "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
        "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
        "    k = 0; # For counting the current row no.\n",
        "    for i in b:\n",
        "        t = 0.0\n",
        "        for j in range(len(i)-1):\n",
        "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "        for j in range(len(i)-2):\n",
        "            t += abs(temp1[j+1]-temp1[j]) # Summing the 2nd Diffs\n",
        "        output[k] = t/(len(i)-2) # Calculating the mean of the 2nd Diffs\n",
        "        k +=1 # Updating the current row no.\n",
        "    return np.sum(output)/tr"
      ],
      "metadata": {
        "id": "oMAOrOv2_0Xb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "second difference max"
      ],
      "metadata": {
        "id": "uz8KIZZf_1W8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def secDiffMax(a,tr):\n",
        "    b = a # Extracting the data from the 14 channels\n",
        "    output = np.zeros(len(b)) # Initializing the output array with zeros (length = 14)\n",
        "    temp1 = np.zeros(len(b[0])-1) # To store the 1st Diffs\n",
        "    k = 0; # For counting the current row no.\n",
        "    t = 0.0\n",
        "    for i in b:\n",
        "        for j in range(len(i)-1):\n",
        "            temp1[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "        t = temp1[1] - temp1[0]\n",
        "        for j in range(len(i)-2):\n",
        "            if abs(temp1[j+1]-temp1[j]) > t :\n",
        "                t = temp1[j+1]-temp1[j] # Comparing current Diff with the last updated Diff Max\n",
        "\n",
        "        output[k] = t # Storing the 2nd Diff Max for channel k\n",
        "        k +=1 # Updating the current row no.\n",
        "    return np.sum(output)/tr"
      ],
      "metadata": {
        "id": "oHW5ae2r_5Vd"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "skewness"
      ],
      "metadata": {
        "id": "flR76sKi_53E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as sp\n",
        "def skewness(arr,tr):\n",
        "    data = arr \n",
        "    skew_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
        "    index = 0; #current cell position in the output array\n",
        "   \n",
        "    for i in data:\n",
        "        skew_array[index]=sp.stats.skew(i,axis=0,bias=True)\n",
        "        index+=1 #updating the cell position\n",
        "    return np.sum(skew_array)/tr"
      ],
      "metadata": {
        "id": "qvM9mpz-_9SU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first difference mean"
      ],
      "metadata": {
        "id": "wHqi7ymt_-Ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_diff_mean(arr,tr):\n",
        "    data = arr \n",
        "    diff_mean_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
        "    index = 0; #current cell position in the output array\n",
        "   \n",
        "    for i in data:\n",
        "        sum=0.0#initializing the sum at the start of each iteration\n",
        "        for j in range(len(i)-1):\n",
        "            sum += abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "           \n",
        "        diff_mean_array[index]=sum/(len(i)-1)\n",
        "        index+=1 #updating the cell position\n",
        "    return np.sum(diff_mean_array)/tr"
      ],
      "metadata": {
        "id": "cHpPbvnOABaM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "first difference max"
      ],
      "metadata": {
        "id": "qFV8Xh-LACcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_diff_max(arr,tr):\n",
        "    data = arr \n",
        "    diff_max_array = np.zeros(len(data)) #Initialinling the array as all 0s\n",
        "    first_diff = np.zeros(len(data[0])-1)#Initialinling the array as all 0s \n",
        "    index = 0; #current cell position in the output array\n",
        "   \n",
        "    for i in data:\n",
        "        max=0.0#initializing at the start of each iteration\n",
        "        for j in range(len(i)-1):\n",
        "            first_diff[j] = abs(i[j+1]-i[j]) # Obtaining the 1st Diffs\n",
        "            if first_diff[j]>max: \n",
        "                max=first_diff[j] # finding the maximum of the first differences\n",
        "        diff_max_array[index]=max\n",
        "        index+=1 #updating the cell position\n",
        "    return np.sum(diff_max_array)/tr"
      ],
      "metadata": {
        "id": "mLjl-ZLJALK0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wavelet Fetures! : \n",
        "Approx Mean, Approx Std Deviation, Approx Energy, Detailed Mean, Detailed Std Deviation, Detailed Energy, Approx Entropy & Detailed Entropy"
      ],
      "metadata": {
        "id": "cSGTzPS2ANhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pywt\n",
        "\n",
        "def wavelet_features(epoch,channels):\n",
        "    cA_values = []\n",
        "    cD_values = []\n",
        "    cA_mean = []\n",
        "    cA_std = []\n",
        "    cA_Energy =[]\n",
        "    cD_mean = []\n",
        "    cD_std = []\n",
        "    cD_Energy = []\n",
        "    Entropy_D = [] \n",
        "    Entropy_A = []\n",
        "    wfeatures = []\n",
        "    for i in range(channels):\n",
        "        cA,cD=pywt.dwt(epoch[i,:],'coif1')\n",
        "        cA_values.append(cA)\n",
        "        cD_values.append(cD)\t\t#calculating the coefficients of wavelet transform.\n",
        "    for x in range(channels):   \n",
        "        cA_mean.append(np.mean(cA_values[x]))\n",
        "        wfeatures.append(np.mean(cA_values[x]))\n",
        "        \n",
        "        cA_std.append(abs(np.std(cA_values[x])))\n",
        "        wfeatures.append(abs(np.std(cA_values[x])))\n",
        "        \n",
        "        cA_Energy.append(abs(np.sum(np.square(cA_values[x]))))\n",
        "        wfeatures.append(abs(np.sum(np.square(cA_values[x]))))\n",
        "        \n",
        "        cD_mean.append(np.mean(cD_values[x]))\t\t# mean and standard deviation values of coefficents of each channel is stored .\n",
        "        wfeatures.append(np.mean(cD_values[x]))\n",
        "\n",
        "        cD_std.append(abs(np.std(cD_values[x])))\n",
        "        wfeatures.append(abs(np.std(cD_values[x])))\n",
        "        \n",
        "        cD_Energy.append(abs(np.sum(np.square(cD_values[x]))))\n",
        "        wfeatures.append(abs(np.sum(np.square(cD_values[x]))))\n",
        "        \n",
        "        Entropy_D.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
        "        wfeatures.append(abs(np.sum(np.square(cD_values[x]) * np.log(np.square(cD_values[x])))))\n",
        "        \n",
        "        Entropy_A.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x]))))) \n",
        "        wfeatures.append(abs(np.sum(np.square(cA_values[x]) * np.log(np.square(cA_values[x])))))\n",
        "    return wfeatures"
      ],
      "metadata": {
        "id": "Otz5OfnlAQeb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FFT Max Power - Delta, Theta, Alpha & Beta Band!"
      ],
      "metadata": {
        "id": "_plwYS9tE1Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import signal\n",
        "\n",
        "def maxPwelch(data_win,Fs,tr):\n",
        " \n",
        "    \n",
        "    BandF = [0.1, 3, 7, 12, 30]\n",
        "    PMax = np.zeros([tr,(len(BandF)-1)]);\n",
        "    \n",
        "    for j in range(14):\n",
        "        f,Psd = signal.welch(data_win[j,:], Fs)\n",
        "        \n",
        "        for i in range(len(BandF)-1):\n",
        "            fr = np.where((f>BandF[i]) & (f<=BandF[i+1]))\n",
        "            PMax[j,i] = np.max(Psd[fr])\n",
        "    \n",
        "    return np.sum(PMax[:,0])/tr,np.sum(PMax[:,1])/tr,np.sum(PMax[:,2])/tr,np.sum(PMax[:,3])/tr"
      ],
      "metadata": {
        "id": "Y7OvEtHtE13_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "hurst exponent"
      ],
      "metadata": {
        "id": "bD40cOj_rwVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hurst(epoch,tr):\n",
        "    hurst = np.zeros(len(epoch))\n",
        "    index = 0\n",
        "    for X in epoch:\n",
        "      H =  nolds.hurst_rs(X)\n",
        "      hurst[index] = H\n",
        "    return np.sum(hurst)/tr"
      ],
      "metadata": {
        "id": "wK-n4KexPR5G"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sample entropy"
      ],
      "metadata": {
        "id": "bMfS3qlAPSg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_entropy(epoch,tr):\n",
        "    sam_en = np.zeros(len(epoch))\n",
        "    index = 0\n",
        "    for X in epoch:\n",
        "      S =  nolds.sampen(X)\n",
        "      sam_en[index] = S\n",
        "      index = index +1\n",
        "    return np.sum(sam_en)/tr"
      ],
      "metadata": {
        "id": "CX9YFlzNrrpv"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def permutation_entropy(epoch,tr):\n",
        "    per_en = np.zeros(len(epoch))\n",
        "    index = 0\n",
        "    for X in epoch:\n",
        "      S =  ent.permutation_entropy(X, 4, 1)\n",
        "      per_en[index] = S\n",
        "      index = index +1\n",
        "    return np.sum(per_en)/tr"
      ],
      "metadata": {
        "id": "TRwDrOR4VFdX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shanon Entropy\n"
      ],
      "metadata": {
        "id": "1dNyuxEFF73N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def s_entropy(epochs,tr): \n",
        "  # Shanon Entropy\n",
        "  \"\"\" Computes entropy of 0-1 vector. \"\"\"\n",
        "  sh_entropies = np.zeros(len(epochs))\n",
        "  index =0\n",
        "  for labels in epochs:\n",
        "    n_labels = len(labels)\n",
        "    labels = list(labels)\n",
        "    counts = np.bincount(labels)\n",
        "    probs = counts[np.nonzero(counts)] / n_labels\n",
        "    n_classes = len(probs)\n",
        "    if n_classes <= 1:\n",
        "      sh_entropy = 0\n",
        "    else:\n",
        "      sh_entropy =  - np.sum(probs * np.log(probs)) / np.log(n_classes)\n",
        "    sh_entropies[index] = sh_entropy\n",
        "    index = index +1\n",
        "  return np.sum(sh_entropies)/tr\n"
      ],
      "metadata": {
        "id": "V1bSPBZ6GEjV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detrended Fluctuation Analysis"
      ],
      "metadata": {
        "id": "WIpl0Z-iiqGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_analysis(epoch,tr):\n",
        "    df_an = np.zeros(len(epoch))\n",
        "    index = 0\n",
        "    for X in epoch:\n",
        "      S = nolds.dfa(X)\n",
        "      df_an[index] = S\n",
        "      index = index +1\n",
        "    return np.sum(df_an)/tr"
      ],
      "metadata": {
        "id": "mkXmhcdbitNt"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spectral Entropy"
      ],
      "metadata": {
        "id": "rAjsoEBoGFP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.fft import fft\n",
        "from numpy import zeros, floor, log10, log, mean, array, sqrt, vstack, cumsum, ones, log2, std\n",
        "from numpy.linalg import svd, lstsq\n",
        "import time\n",
        "\n",
        "def bin_power(X,Band,Fs):\n",
        "    C = fft(X)\n",
        "    C = abs(C)\n",
        "    Power =zeros(len(Band)-1);\n",
        "    for Freq_Index in range(0,len(Band)-1):\n",
        "        Freq = float(Band[Freq_Index])   ## Xin Liu\n",
        "        Next_Freq = float(Band[Freq_Index+1])\n",
        "        print(\"\")\n",
        "        Power[Freq_Index] = sum(C[floor(Freq/Fs*len(X)):floor(Next_Freq/Fs*len(X))])\n",
        "    Power_Ratio = Power/sum(Power)\n",
        "    return Power, Power_Ratio\n",
        "\n",
        "\n",
        "def spectral_entropy(X, Fs, Power_Ratio = None):\n",
        "    \n",
        "    Band = [0.1, 3, 7, 12, 30]\n",
        "    if Power_Ratio is None:\n",
        "        Power, Power_Ratio = bin_power(X, Band, Fs)\n",
        "\n",
        "    Spectral_Entropy = 0\n",
        "    for i in range(0, len(Power_Ratio) - 1):\n",
        "        Spectral_Entropy += Power_Ratio[i] * log(Power_Ratio[i])\n",
        "    Spectral_Entropy /= log(len(Power_Ratio))     # to save time, minus one is omitted\n",
        "    print('Shape of Spectral Entropy = ',n.shape(Spectral_Entropy))\n",
        "    return -1 * Spectral_Entropy"
      ],
      "metadata": {
        "id": "mzBkHPtzGNS-"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Coherence"
      ],
      "metadata": {
        "id": "sbRWgOPMlCte"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coh_res = []\n",
        "def coherence(eegData):\n",
        "  # for eegData in epoch:\n",
        "    coh_res = []\n",
        "    print(\"eegData.shape[0] >>>>>>\",eegData.shape[0])\n",
        "    # for ii, jj in itertools.combinations(range(eegData.shape[0]), 2):\n",
        "    #   coh_res.append(CoherenceDelta(eegData, ii, jj, fs=128))\n",
        "\n",
        "    # coh_res = np.array(coh_res)\n",
        "    # print(\"coh_res >>>>>>>>>>>>.\",coh_res)\n",
        "    return coh_res\n",
        "\n",
        "# Coherence in the Delta Band\n",
        "def CoherenceDelta(eegData, i, j, fs=128):\n",
        "    print(\"eegData >>>>>>>>>>\",eegData)\n",
        "    print(\"ii >>>\",i)\n",
        "    print(\"jj >>>\",j)\n",
        "    nfft=eegData.shape[1]\n",
        "    print(\"nfft >>\",nfft)\n",
        "    f, Cxy = signal.coherence(eegData[i,:], eegData[j,:], fs=fs, nfft=nfft, axis=0)#, window=np.hanning(nfft))\n",
        "    out = np.mean(Cxy[np.all([f >= 0.5, f<=4], axis=0)], axis=0)\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pl8KGJQllD9V"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "phase coherence"
      ],
      "metadata": {
        "id": "MaGQBCteOUsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PhaseCoherence(freq, timeSeries, FS):\n",
        "\n",
        "    # Get parameters of input data\n",
        "    nMeasures\t = np.shape(timeSeries)[0]\n",
        "    nSamples \t= np.shape(timeSeries)[1]\n",
        "    nSecs = nSamples / FS\n",
        "    print('Number of measurements =', nMeasures)\n",
        "    print('Number of time samples =', nSamples, '=', nSecs, 'seconds')\n",
        "    \n",
        "    # Calculate FFT for each measurement (spect is freq x measurements)\n",
        "    spect = np.fft.fft(timeSeries, axis=1)\n",
        "    \n",
        "    # Normalise by amplitude\n",
        "    spect = spect / abs(spect)\n",
        "    \n",
        "    # Find spectrum values for frequency bin of interest\n",
        "    freqRes = 1 / nSecs;\n",
        "    foibin = round(freq / freqRes + 1) - 1\n",
        "    spectFoi = spect[:,foibin]\n",
        "    \n",
        "    # Find individual phase angles per measurement at frequency of interest\n",
        "    anglesFoi = np.arctan2(spectFoi.imag, spectFoi.real)\n",
        "    \n",
        "    # PC is root mean square of the sums of the cosines and sines of the angles\n",
        "    PC = np.sqrt((np.sum(np.cos(anglesFoi)))**2 + (np.sum(np.sin(anglesFoi)))**2) / np.shape(anglesFoi)[0]\n",
        "    \n",
        "    # Print the value\n",
        "    print('----------------------------------');\n",
        "    print('Phase coherence value = ' + str(\"{0:.3f}\".format(PC)));\n",
        "        \n",
        "    return PC"
      ],
      "metadata": {
        "id": "Lb4nGdO7OWyf"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lyapunov exponents "
      ],
      "metadata": {
        "id": "ma8krPoes2nd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LLE(x, tau, n, T, fs):\n",
        "    \"\"\"Calculate largest Lyauponov exponent of a given time series x using\n",
        "    Rosenstein algorithm.\n",
        "    Parameters\n",
        "    ----------\n",
        "    x\n",
        "        list\n",
        "        a time series\n",
        "    n\n",
        "        integer\n",
        "        embedding dimension\n",
        "    tau\n",
        "        integer\n",
        "        Embedding lag\n",
        "    fs\n",
        "        integer\n",
        "        Sampling frequency\n",
        "    T\n",
        "        integer\n",
        "        Mean period\n",
        "    Returns\n",
        "    ----------\n",
        "    Lexp\n",
        "       float\n",
        "       Largest Lyapunov Exponent\n",
        "    Notes\n",
        "    ----------\n",
        "    A n-dimensional trajectory is first reconstructed from the observed data by\n",
        "    use of embedding delay of tau, using pyeeg function, embed_seq(x, tau, n).\n",
        "    Algorithm then searches for nearest neighbour of each point on the\n",
        "    reconstructed trajectory; temporal separation of nearest neighbours must be\n",
        "    greater than mean period of the time series: the mean period can be\n",
        "    estimated as the reciprocal of the mean frequency in power spectrum\n",
        "    Each pair of nearest neighbours is assumed to diverge exponentially at a\n",
        "    rate given by largest Lyapunov exponent. Now having a collection of\n",
        "    neighbours, a least square fit to the average exponential divergence is\n",
        "    calculated. The slope of this line gives an accurate estimate of the\n",
        "    largest Lyapunov exponent.\n",
        "    References\n",
        "    ----------\n",
        "    Rosenstein, Michael T., James J. Collins, and Carlo J. De Luca. \"A\n",
        "    practical method for calculating largest Lyapunov exponents from small data\n",
        "    sets.\" Physica D: Nonlinear Phenomena 65.1 (1993): 117-134.\n",
        "    Examples\n",
        "    ----------\n",
        "    >>> import pyeeg\n",
        "    >>> X = numpy.array([3,4,1,2,4,51,4,32,24,12,3,45])\n",
        "    >>> pyeeg.LLE(X,2,4,1,1)\n",
        "       0.18771136179353307\n",
        "    \"\"\"\n",
        "\n",
        "    from embedded_sequence import embed_seq\n",
        "\n",
        "    Em = embed_seq(x, tau, n)\n",
        "    M = len(Em)\n",
        "    A = numpy.tile(Em, (len(Em), 1, 1))\n",
        "    B = numpy.transpose(A, [1, 0, 2])\n",
        "\n",
        "    #  square_dists[i,j,k] = (Em[i][k]-Em[j][k])^2\n",
        "    square_dists = (A - B) ** 2\n",
        "\n",
        "    #  D[i,j] = ||Em[i]-Em[j]||_2\n",
        "    D = numpy.sqrt(square_dists[:, :, :].sum(axis=2))\n",
        "\n",
        "    # Exclude elements within T of the diagonal\n",
        "    band = numpy.tri(D.shape[0], k=T) - numpy.tri(D.shape[0], k=-T - 1)\n",
        "    band[band == 1] = numpy.inf\n",
        "\n",
        "    # nearest neighbors more than T steps away\n",
        "    neighbors = (D + band).argmin(axis=0)\n",
        "\n",
        "    # in_bounds[i,j] = (i+j <= M-1 and i+neighbors[j] <= M-1)\n",
        "    inc = numpy.tile(numpy.arange(M), (M, 1))\n",
        "    row_inds = (numpy.tile(numpy.arange(M), (M, 1)).T + inc)\n",
        "    col_inds = (numpy.tile(neighbors, (M, 1)) + inc.T)\n",
        "    in_bounds = numpy.logical_and(row_inds <= M - 1, col_inds <= M - 1)\n",
        "\n",
        "    # Uncomment for old (miscounted) version\n",
        "    # in_bounds = numpy.logical_and(row_inds < M - 1, col_inds < M - 1)\n",
        "    row_inds[~in_bounds] = 0\n",
        "    col_inds[~in_bounds] = 0\n",
        "\n",
        "    # neighbor_dists[i,j] = ||Em[i+j]-Em[i+neighbors[j]]||_2\n",
        "    neighbor_dists = numpy.ma.MaskedArray(D[row_inds, col_inds], ~in_bounds)\n",
        "\n",
        "    #  number of in-bounds indices by row\n",
        "    J = (~neighbor_dists.mask).sum(axis=1)\n",
        "\n",
        "    # Set invalid (zero) values to 1; log(1) = 0 so sum is unchanged\n",
        "    neighbor_dists[neighbor_dists == 0] = 1\n",
        "    d_ij = numpy.sum(numpy.log(neighbor_dists.data), axis=1)\n",
        "    mean_d = d_ij[J > 0] / J[J > 0]\n",
        "\n",
        "    x = numpy.arange(len(mean_d))\n",
        "    X = numpy.vstack((x, numpy.ones(len(mean_d)))).T\n",
        "    [m, c] = numpy.linalg.lstsq(X, mean_d)[0]\n",
        "    Lexp = fs * m\n",
        "    return Lexp\n"
      ],
      "metadata": {
        "id": "B5unNk0ws3Wl"
      },
      "execution_count": 36,
      "outputs": []
    }
  ]
}